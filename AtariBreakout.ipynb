{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Learn Atari Breakout\n",
    "\n",
    "This notebook contains the source code from the YouTube video [Neural Network Learns To Play Atari Breakout](url_here) and walks you through how to define a neural network in Tensorflow 2.0 and Pytorch and train it to learn to play Atari Breakout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Import the libraries that are needed for loading the Breakout Unity game ([mlagents](https://github.com/Unity-Technologies/ml-agents)), defining the Tensorflow 2.0 neural network ([tensorflow](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf)) and the Pytorch version ([torch](https://pytorch.org/docs/stable/index.html)). Note that the ML-Agents library requires [Python 3.6](https://www.python.org/downloads/release/python-368/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q mlagents==0.7\n",
    "!pip3 install -q tensorflow==2.0.0-alpha0\n",
    "!pip3 install -q torch==1.0.1.post2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Unity Environment\n",
    "\n",
    "The next step is to load the Breakout environment as a `UnityEnvironment` object which launches and begins communication with the Unity game executable when instantiated. The `unity_env` variable is the handle for sending agent actions to the game and receiving resulting next/terminal states and rewards at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 2\n",
      "        Number of Training Brains : 1\n",
      "        Reset Parameters :\n",
      "\t\tepisode -> 0.0\n",
      "Unity brain name: BreakoutLearning\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space size (per agent): 54\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): [3]\n",
      "        Vector Action descriptions: \n",
      "Unity brain name: BreakoutPlayer\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space size (per agent): 54\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): [3]\n",
      "        Vector Action descriptions: \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import numpy as np\n",
    "from mlagents.envs import UnityEnvironment\n",
    "\n",
    "unity_files = {\"Linux\": \"./env/linux/Breakout\", \"Darwin\": \"./env/mac/Breakout\", \"Windows\": \"./env/windows/Breakout\"}\n",
    "unity_file = env_names[platform.system()]\n",
    "unity_env = UnityEnvironment(file_name=unity_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create OpenAI Gym Wrapper for Unity Environment\n",
    "\n",
    "As most people are more familiar with the gym API of [OpenAI Gym](https://gym.openai.com/docs/) for communicating with an environment, we will create a wrapper class `GymEnvironment` that mimics the OpenAI Gym environment functions for resetting and stepping through an environment. You can also do any preprocessing of states returned from the environment or adjust the reward signal.\n",
    "\n",
    "- For more info, check out this tutorial video for [Getting Started with OpenAI Gym](https://www.youtube.com/watch?v=8MC3y7ASoPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GymEnvironment():\n",
    "    def __init__(self, unity_env):\n",
    "        self.env = unity_env\n",
    "        self.default_brain = unity_env.brain_names[0]\n",
    "        self.observation_space_size = unity_env.brains[self.default_brain].vector_observation_space_size\n",
    "        self.action_space_size = int(unity_env.brains[self.default_brain].vector_action_space_size[0])\n",
    "        \n",
    "    def reset(self, train_mode=True):\n",
    "        self.env_info = self.env.reset(train_mode=train_mode)[self.default_brain]\n",
    "        return self.env_info.vector_observations[0]\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.env_info = self.env.step(action)[self.default_brain]\n",
    "        next_state = self.env_info.vector_observations[0]\n",
    "        reward = self.env_info.rewards[0]\n",
    "        done = self.env_info.local_done[0]\n",
    "        return next_state, reward, done, self.env_info\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Understand the Breakout Environment\n",
    "\n",
    "We can reset the environment to be provided with an initial set of states for the agent in the environment. The `action_size` variable holds the number of actions that the agent can choose from. _states_ refer to a vector of variables corresponding to relevant aspects of the environment that the agent 'sees'.\n",
    "\n",
    "\n",
    "![AtariBreakout](./AtariBreakout.png)\n",
    "\n",
    "\n",
    "In the breakout environment, the agent is controlling a paddle at the bottom of the screen and can between 3 actions: to move it left, right or no movement. Its goal is to hit the ball to destroy all the floating bricks and prevent the ball from falling to the bottom. It will get a reward of +1 whenever the ball hits a brick and -1 for each brick remaining if the ball falls to the bottom.\n",
    "\n",
    "The state consists of 54 numbers where the first two are the ball's `x` and `y` position, the next two are the ball's `x` and `y` velocity, the next two are the paddle's `x` position and `x` velocity and the remaining 48 represent each brick's status of destroyed (0.) or remaining (1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 3\n",
      "States have length: 54\n",
      "States look like: [ 0.          0.         -2.77063155  5.3219924   0.          0.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.        ]\n"
     ]
    }
   ],
   "source": [
    "env = GymEnvironment(unity_env)\n",
    "action_size = env.action_space_size\n",
    "state_size = env.observation_space_size\n",
    "state = env.reset()\n",
    "\n",
    "print('Number of actions:', action_size)\n",
    "print('States have length:', state_size)\n",
    "print('States look like:', state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the Reinforcement Learning Components\n",
    "\n",
    "The next step is to define the components that make up the reinforcement learning algorithm. This requires defining:\n",
    "\n",
    "- **3.1** The neural network architecture to approximate the Q function.\n",
    "- **3.2** The experience replay buffer for storing and sampling batches of experience tuples.\n",
    "- **3.3** The Q-Learning agent that samples from the replay buffer and trains the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.0 Neural Network Hyperparameters\n",
    "\n",
    "These are the settings for the neural network that we need to tweak to suit our particular task of playing Breakout. \n",
    "- `HIDDEN_SIZE` specifies the number of nodes in each layer of the network. \n",
    "- `LEARNING_RATE` sets how much we want to update the network weights at each training step. \n",
    "- `REGULARIZER_LAMBDA` is a penalty multiplier to apply for the size of the network weights.\n",
    "- `TARGET_UPDATE_RATE` is how frequently we want to copy the local network to the target network (for double DQNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 800\n",
    "LEARNING_RATE = 0.00002\n",
    "REGULARIZER_LAMBDA = 1e-6\n",
    "TARGET_UPDATE_RATE = 0.0004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3.1.1 Tensorflow QNetwork\n",
    "\n",
    "Below is the implementation of a Deep Q Network using TensorFlow 2.0 as the backend model. The `TFModel` class defines the actual neural network architecture for approximating the Q-function and the `TFQNetwork` incorporates the neural network to define a function for retrieving the output q-values from an input state (`get_q_state`) and also a function for training the neural network (`optimize`).\n",
    "\n",
    "Resources:\n",
    "- A tutorial video on [Getting Started with Tensorflow 2.0](https://www.youtube.com/watch?v=fQCKxzHvYnw)\n",
    "- A tutorial video on [Basic Deep Q Network (DQN) in Tensorflow](https://www.youtube.com/watch?v=dpBKz1wxE_c)\n",
    "- A tutorial video on [Upgrading a DQN to a Double DQN](https://www.youtube.com/watch?v=ILDLT97FsNM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow: 2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow:\", tf.__version__)\n",
    "\n",
    "class TFModel(tf.keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        self.hidden1 = tf.keras.layers.Dense(HIDDEN_SIZE, activation=tf.nn.relu, kernel_initializer=tf.initializers.glorot_normal())\n",
    "        self.hidden2 = tf.keras.layers.Dense(HIDDEN_SIZE, activation=tf.nn.relu, kernel_initializer=tf.initializers.glorot_normal(), kernel_regularizer=tf.keras.regularizers.l2(l=REGULARIZER_LAMBDA))\n",
    "        self.q_state = tf.keras.layers.Dense(action_size, activation=None, kernel_initializer=tf.initializers.glorot_normal())\n",
    "        \n",
    "    def call(self, state):\n",
    "        hidden1 = self.hidden1(state)\n",
    "        hidden2 = self.hidden2(hidden1) + hidden1\n",
    "        q_state = self.q_state(hidden2)\n",
    "        return q_state\n",
    "    \n",
    "class TFQNetwork():\n",
    "    def __init__(self, state_size, action_size, load=False):\n",
    "        self.model_local = TFModel(state_size, action_size)\n",
    "        self.model_target = TFModel(state_size, action_size)\n",
    "        self.optimizer = tf.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "        self.action_size = action_size\n",
    "        if load: self.load_model()\n",
    "        \n",
    "    def get_q_state(self, state, use_target=False):\n",
    "        model = self.model_local if not use_target else self.model_target\n",
    "        return model(np.array(state)).numpy()\n",
    "    \n",
    "    def get_loss(self, states, actions, q_targets):\n",
    "        actions_one_hot = tf.one_hot(actions, depth=self.action_size)\n",
    "        q_states = tf.cast(self.model_local(states), tf.float32)\n",
    "        q_states_actions = tf.reduce_sum(tf.multiply(q_states, actions_one_hot), axis=1)\n",
    "        loss = tf.reduce_sum(tf.square(q_states_actions - q_targets))\n",
    "        return loss\n",
    "    \n",
    "    def optimize(self, states, actions, q_targets):\n",
    "        loss = lambda: self.get_loss(states, actions, q_targets)\n",
    "        self.optimizer.minimize(loss=loss, var_list=self.model_local.trainable_weights)\n",
    "        self.soft_copy(self.model_local, self.model_target)\n",
    "        \n",
    "    def soft_copy(self, local, target, tau=TARGET_UPDATE_RATE):\n",
    "        new_target_vars = [t + tau*(l-t) for l,t in zip(local.get_weights(), target.get_weights())]\n",
    "        self.model_target.set_weights(new_target_vars)\n",
    "        \n",
    "    def save_model(self, filepath=\"./saved_models/tensorflow2/model.tf\"):\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        self.model_local.save_weights(filepath)\n",
    "        \n",
    "    def load_model(self, filepath=\"./saved_models/tensorflow2/model.tf\"):\n",
    "        if os.path.exists(filepath + \".index\"):\n",
    "            self.model_local.load_weights(filepath)\n",
    "            self.model_target.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 PyTorch QNetwork\n",
    "\n",
    "Below is the implementation of a QNetwork using PyTorch as the backend model. The `PTModel` class defines the actual neural network architecture for approximating the Q-function and the `PTQNetwork` is the equivalent of the `TFQNetwork` adapted for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "\n",
    "class PTModel(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        self.hidden1 = torch.nn.Linear(state_size, HIDDEN_SIZE)\n",
    "        self.hidden2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        self.q_state = torch.nn.Linear(HIDDEN_SIZE, action_size)\n",
    "        torch.nn.init.xavier_normal_(self.hidden1.weight)\n",
    "        torch.nn.init.xavier_normal_(self.hidden2.weight)\n",
    "        torch.nn.init.xavier_normal_(self.q_state.weight)\n",
    "\n",
    "    def forward(self, state):\n",
    "        hidden1 = torch.nn.functional.relu(self.hidden1(state))\n",
    "        hidden2 = torch.nn.functional.relu(self.hidden2(hidden1)) + hidden1\n",
    "        q_state = self.q_state(hidden2)\n",
    "        return q_state\n",
    "\n",
    "class PTQNetwork():\n",
    "    def __init__(self, state_size, action_size, load=False):\n",
    "        self.model_local = PTModel(state_size, action_size)\n",
    "        self.model_target = PTModel(state_size, action_size)\n",
    "        self.optimizer = torch.optim.Adam(self.model_local.parameters(), lr=LEARNING_RATE, weight_decay=REGULARIZER_LAMBDA)\n",
    "        if load: self.load_model()\n",
    "\n",
    "    def get_q_state(self, state, use_target=False):\n",
    "        model = self.model_local if not use_target else self.model_target\n",
    "        state = torch.from_numpy(np.array(state)).float()\n",
    "        return model(state).detach().numpy()\n",
    "    \n",
    "    def get_loss(self, states, actions, q_targets):\n",
    "        states = torch.from_numpy(np.vstack(states)).float()\n",
    "        actions = torch.from_numpy(np.vstack(actions)).long()\n",
    "        q_targets = torch.from_numpy(np.vstack(q_targets)).float()\n",
    "        q_states_actions = self.model_local(states).gather(1, actions)\n",
    "        loss = (q_states_actions - q_targets)**2\n",
    "        return loss.mean()\n",
    "    \n",
    "    def optimize(self, states, actions, q_targets):\n",
    "        loss = self.get_loss(states, actions, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.soft_copy(self.model_local, self.model_target)\n",
    "        \n",
    "    def soft_copy(self, local, target, tau=TARGET_UPDATE_RATE):\n",
    "        for l,t in zip(local.parameters(), target.parameters()):\n",
    "            t.data.copy_(t.data + tau*(l.data - t.data))\n",
    "        \n",
    "    def save_model(self, filepath=\"./saved_models/pytorch/checkpoint.pth\"):\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        torch.save(self.model_local.state_dict(), filepath)\n",
    "        \n",
    "    def load_model(self, filepath=\"./saved_models/pytorch/checkpoint.pth\"):\n",
    "        if os.path.exists(filepath):\n",
    "            self.model_local.load_state_dict(torch.load(filepath))\n",
    "            self.model_target.load_state_dict(torch.load(filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Experience Replay\n",
    "\n",
    "Below is the implementation of a Replay Buffer using the `deque` collection as the rolling buffer of experience tuples. This can be sampled by specifying the sample size and then returns each individual experience type as separate lists.\n",
    "\n",
    "- Check out this tutorial video on [How Experience Replay Works](https://www.youtube.com/watch?v=lQB08wcuCeM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, maxlen):\n",
    "        self.buffer = deque(maxlen=maxlen)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        sample_size = min(len(self.buffer), batch_size)\n",
    "        samples = random.choices(self.buffer, k=sample_size)\n",
    "        return map(list, zip(*samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Q-Learning Algorithm\n",
    "\n",
    "This step implements the final AI agent that uses Deep Q Networks to learn the Bellman equation for selecting actions to take in a given state from the environment. It instantiates a QNetwork of the specified implementation (`TFQNetwork` or `PTQNetwork`) as well as the `ReplayBuffer` for experience replay.\n",
    "\n",
    "- Check out this tutorial video for [Q-Learning with the Bellman Equation](https://www.youtube.com/watch?v=wN3rxIKmMgE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.0 Q-Learning Hyperparameters\n",
    "\n",
    "Here we define the settings for the training of the agent.\n",
    "\n",
    "- `MAX_BUFFER_SIZE` sets the maximum length of the replay buffer.\n",
    "- `REPLAY_BATCH_SIZE` is how many experience tuples to sample from the buffer for each train step.\n",
    "- `DISCOUNT_RATE` is the constant _gamma_ for discounting future rewards in the Bellman Equation\n",
    "- `EPS_MAX` is the starting proportion of random to greedy actions to take\n",
    "- `EPS_MIN` is the lower limit proportion of random to greedy actions to take\n",
    "- `EPS_DECAY` is the rate at which eps decays from `EPS_MAX` to `EPS_MIN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_BUFFER_SIZE = 1000000\n",
    "REPLAY_BATCH_SIZE = 32\n",
    "DISCOUNT_RATE = 0.99\n",
    "EPS_MAX = 1.0\n",
    "EPS_MIN = 0.1\n",
    "EPS_DECAY = 0.998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Q-Learning Agent\n",
    "\n",
    "The `get_action` function infers an action from the QNetwork from a given state using an epsilon-greedy policy for choosing a random action more often at the start of training for exploration, and then selecting the greedy action more often later in training as the network's approximation of the Q-Function (Bellman Equation) improves.\n",
    "\n",
    "The `train` function calculates the target Q-value (`q_target`) for the `reward` from the `action` taken in the given `state` and then trains the QNetwork toward that value for the input state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, state_size, action_size, network=TFQNetwork, eps=EPS_MAX, load=False):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.q_network = network(state_size, action_size, load)\n",
    "        self.replay_buffer = ReplayBuffer(MAX_BUFFER_SIZE)\n",
    "        self.gamma = DISCOUNT_RATE\n",
    "        self.eps = eps\n",
    "\n",
    "    def get_action(self, state, eps=None):\n",
    "        eps = self.eps if eps == None else eps\n",
    "        action_greedy = np.argmax(self.q_network.get_q_state([state]))\n",
    "        action_random = np.random.randint(self.action_size)\n",
    "        action = action_random if random.random() < eps else action_greedy\n",
    "        return action\n",
    "        \n",
    "    def train(self, state, action, next_state, reward, done):\n",
    "        self.replay_buffer.add((state, action, next_state, reward, done))\n",
    "        states, actions, next_states, rewards, dones = self.replay_buffer.sample(REPLAY_BATCH_SIZE)\n",
    "        \n",
    "        next_actions = np.argmax(self.q_network.get_q_state(next_states, use_target=False), axis=1)\n",
    "        q_next_states = self.q_network.get_q_state(next_states, use_target=True)\n",
    "        q_next_states[dones] = np.zeros([self.action_size])\n",
    "        q_next_states_next_actions = q_next_states[np.arange(next_actions.shape[0]), next_actions]\n",
    "        q_targets = rewards + self.gamma * q_next_states_next_actions\n",
    "        self.q_network.optimize(states, actions, q_targets)\n",
    "\n",
    "        if done: self.eps = max(self.eps * EPS_DECAY, EPS_MIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Agent\n",
    "\n",
    "Below is the training loop for training the agent through a number of episodes of interacting with the environment. It keeps track of the total reward from each episode and also stores the last 100 episode rewards for calculating the average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1100, Score: 16.0, Avg reward: 2.74\n",
      "Episode: 1101, Score: 26.0, Avg reward: 3.22\n",
      "Episode: 1102, Score: 34.0, Avg reward: 4.04\n",
      "Episode: 1103, Score: -30.0, Avg reward: 3.36\n",
      "Episode: 1104, Score: 32.0, Avg reward: 3.30\n",
      "Episode: 1105, Score: 46.0, Avg reward: 3.36\n",
      "Episode: 1106, Score: -2.0, Avg reward: 3.68\n",
      "Episode: 1107, Score: 46.0, Avg reward: 4.02\n",
      "Episode: 1108, Score: 10.0, Avg reward: 4.60\n",
      "Episode: 1109, Score: 14.0, Avg reward: 4.88\n",
      "Episode: 1110, Score: -18.0, Avg reward: 4.22\n",
      "Episode: 1111, Score: 18.0, Avg reward: 4.72\n",
      "Episode: 1112, Score: 48.0, Avg reward: 4.86\n",
      "Episode: 1113, Score: 46.0, Avg reward: 5.58\n",
      "Episode: 1114, Score: -18.0, Avg reward: 4.92\n",
      "Episode: 1115, Score: 48.0, Avg reward: 5.70\n",
      "Episode: 1116, Score: 40.0, Avg reward: 6.54\n",
      "Episode: 1117, Score: -2.0, Avg reward: 6.64\n",
      "Episode: 1118, Score: 20.0, Avg reward: 6.84\n",
      "Episode: 1119, Score: -2.0, Avg reward: 6.34\n",
      "Episode: 1120, Score: 48.0, Avg reward: 6.84\n",
      "Episode: 1121, Score: 48.0, Avg reward: 7.64\n",
      "Episode: 1122, Score: 40.0, Avg reward: 8.46\n",
      "Episode: 1123, Score: -34.0, Avg reward: 8.20\n",
      "Episode: 1124, Score: -38.0, Avg reward: 8.12\n",
      "Episode: 1125, Score: 34.0, Avg reward: 8.18\n",
      "Episode: 1126, Score: 2.0, Avg reward: 8.48\n",
      "Episode: 1127, Score: 34.0, Avg reward: 9.18\n",
      "Episode: 1128, Score: 48.0, Avg reward: 9.32\n",
      "Episode: 1129, Score: 12.0, Avg reward: 8.96\n",
      "Episode: 1130, Score: 42.0, Avg reward: 9.08\n",
      "Episode: 1131, Score: 26.0, Avg reward: 9.48\n",
      "Episode: 1132, Score: -38.0, Avg reward: 9.50\n",
      "Episode: 1133, Score: 0.0, Avg reward: 9.66\n",
      "Episode: 1134, Score: -48.0, Avg reward: 8.70\n",
      "Episode: 1135, Score: -4.0, Avg reward: 8.48\n",
      "Episode: 1136, Score: 42.0, Avg reward: 8.44\n",
      "Episode: 1137, Score: -28.0, Avg reward: 8.50\n",
      "Episode: 1138, Score: -12.0, Avg reward: 8.42\n",
      "Episode: 1139, Score: 10.0, Avg reward: 8.22\n",
      "Episode: 1140, Score: 38.0, Avg reward: 8.64\n",
      "Episode: 1141, Score: 40.0, Avg reward: 9.10\n",
      "Episode: 1142, Score: -18.0, Avg reward: 9.32\n",
      "Episode: 1143, Score: 40.0, Avg reward: 9.30\n",
      "Episode: 1144, Score: -38.0, Avg reward: 9.34\n",
      "Episode: 1145, Score: -44.0, Avg reward: 8.58\n",
      "Episode: 1146, Score: 28.0, Avg reward: 9.18\n",
      "Episode: 1147, Score: 48.0, Avg reward: 9.44\n",
      "Episode: 1148, Score: 32.0, Avg reward: 10.18\n",
      "Episode: 1149, Score: 44.0, Avg reward: 10.16\n",
      "Episode: 1150, Score: -38.0, Avg reward: 9.44\n",
      "Episode: 1151, Score: -30.0, Avg reward: 8.92\n",
      "Episode: 1152, Score: -30.0, Avg reward: 8.92\n",
      "Episode: 1153, Score: -34.0, Avg reward: 8.32\n",
      "Episode: 1154, Score: 36.0, Avg reward: 9.10\n",
      "Episode: 1155, Score: 28.0, Avg reward: 9.04\n",
      "Episode: 1156, Score: 22.0, Avg reward: 8.82\n",
      "Episode: 1157, Score: -42.0, Avg reward: 8.10\n",
      "Episode: 1158, Score: 36.0, Avg reward: 8.90\n",
      "Episode: 1159, Score: 46.0, Avg reward: 9.76\n",
      "Episode: 1160, Score: 42.0, Avg reward: 9.88\n",
      "Episode: 1161, Score: -42.0, Avg reward: 9.02\n",
      "Episode: 1162, Score: -22.0, Avg reward: 8.92\n",
      "Episode: 1163, Score: 40.0, Avg reward: 9.04\n",
      "Episode: 1164, Score: -42.0, Avg reward: 8.84\n",
      "Episode: 1165, Score: 10.0, Avg reward: 8.90\n",
      "Episode: 1166, Score: 12.0, Avg reward: 8.94\n",
      "Episode: 1167, Score: 44.0, Avg reward: 9.62\n",
      "Episode: 1168, Score: -16.0, Avg reward: 9.24\n",
      "Episode: 1169, Score: 48.0, Avg reward: 9.76\n",
      "Episode: 1170, Score: -30.0, Avg reward: 9.50\n",
      "Episode: 1171, Score: 42.0, Avg reward: 9.54\n",
      "Episode: 1172, Score: 38.0, Avg reward: 9.58\n",
      "Episode: 1173, Score: 38.0, Avg reward: 9.74\n",
      "Episode: 1174, Score: 48.0, Avg reward: 9.98\n",
      "Episode: 1175, Score: 18.0, Avg reward: 10.54\n",
      "Episode: 1176, Score: 48.0, Avg reward: 10.64\n",
      "Episode: 1177, Score: 38.0, Avg reward: 10.60\n",
      "Episode: 1178, Score: -30.0, Avg reward: 10.06\n",
      "Episode: 1179, Score: 0.0, Avg reward: 9.60\n",
      "Episode: 1180, Score: 48.0, Avg reward: 9.66\n",
      "Episode: 1181, Score: 32.0, Avg reward: 9.58\n",
      "Episode: 1182, Score: 38.0, Avg reward: 10.40\n",
      "Episode: 1183, Score: 42.0, Avg reward: 11.30\n",
      "Episode: 1184, Score: 48.0, Avg reward: 12.12\n",
      "Episode: 1185, Score: 8.0, Avg reward: 12.66\n"
     ]
    }
   ],
   "source": [
    "networks = [TFQNetwork, PTQNetwork]\n",
    "agent = DQNAgent(state_size, action_size, network=networks[0], eps=1.0, load=False)\n",
    "\n",
    "scores = []\n",
    "scores_buffer = deque(maxlen=100)\n",
    "avg_scores = []\n",
    "num_episodes = 2000\n",
    "for ep in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.train(state, action, next_state, reward, done)\n",
    "        total_reward += reward  \n",
    "        state = next_state \n",
    "\n",
    "    scores.append(total_reward)\n",
    "    scores_buffer.append(total_reward)\n",
    "    avg_scores.append(np.mean(scores_buffer))\n",
    "    if avg_scores[-1] >= np.max(avg_scores): agent.q_network.save_model()\n",
    "    print(\"Episode: {}, Score: {}, Avg reward: {:.2f}\".format(ep, scores[-1], avg_scores[-1]))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Plotting the training rewards\n",
    "\n",
    "The plot below shows the total reward from each episode (blue) and the average reward over the last 100 episodes (orange). We can see that the agent was able to reach the average reward of 13 after about 600 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f258b3c0a58>,\n",
       " <matplotlib.lines.Line2D at 0x7f258b3c0cc0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f259e4f4e10>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = range(len(scores))\n",
    "plt.plot(x,scores, x,avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Agent\n",
    "\n",
    "The cell below creates a test agent loading the trained weights for a test run of the environment with the trained model. Change the num_episodes variable to run the simulation and calculate the average reward total over multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Score: 18\n"
     ]
    }
   ],
   "source": [
    "test_agent = DQNAgent(state_size, action_size, load=True)\n",
    "\n",
    "for ep in range(1):\n",
    "    state = env.reset(train_mode=False)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = test_agent.get_action(state, eps=0.0)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward  \n",
    "        state = next_state \n",
    "    print(\"Episode: {}, Score: {}\".format(ep, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Close the environment when finished\n",
    "When we are finished using an environment, we can close it with the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
