{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Learn Atari Breakout\n",
    "\n",
    "This notebook walks you through the setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Import the libraries that are needed for loading the environment. The following loads the necessary dependencies and checks the Python version (at runtime). ML-Agents Toolkit (v0.3 onwards) requires Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q ./ml-agents/ml-agents || pip install -q ./ml-agents/ml-agents/\n",
    "!pip3 install -q tensorflow==2.0.0-alpha0 || pip install -q tensorflow==2.0.0-alpha0\n",
    "# !pip3 install -q torch==1.0.1.post2 || pip install -q torch==1.0.1.post2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the environment\n",
    "\n",
    "`UnityEnvironment` launches and begins communication with the environment when instantiated.\n",
    "\n",
    "Environments contain _brains_ which are responsible for deciding the actions of their associated _agents_. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 2\n",
      "        Number of Training Brains : 1\n",
      "        Reset Parameters :\n",
      "\t\tepisode -> 0.0\n",
      "Unity brain name: BreakoutLearning\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space size (per agent): 54\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): [3]\n",
      "        Vector Action descriptions: \n",
      "Unity brain name: BreakoutPlayer\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space size (per agent): 54\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): [3]\n",
      "        Vector Action descriptions: \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import numpy as np\n",
    "from mlagents.envs import UnityEnvironment\n",
    "\n",
    "env_names = {\"Linux\": \"./env/linux/test/Breakout\", \"Darwin\": \"./env/mac/Breakout\", \"Windows\": \"./env/windows/Breakout\"}\n",
    "env_name = env_names[platform.system()]\n",
    "unity_env = UnityEnvironment(file_name=env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Examine the observation and state spaces\n",
    "\n",
    "We can reset the environment to be provided with an initial set of observations and states for all the agents within the environment. In ML-Agents, _states_ refer to a vector of variables corresponding to relevant aspects of the environment for an agent. Likewise, _observations_ refer to a set of relevant pixel-wise visuals for an agent.\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector must be a number between -1 and 1.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 3\n",
      "States have length: 54\n",
      "States look like: [ 0.          0.         -2.77063155  5.3219924   0.          0.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.        ]\n"
     ]
    }
   ],
   "source": [
    "class GymEnv():\n",
    "    def __init__(self, unity_env):\n",
    "        self.env = unity_env\n",
    "        self.default_brain = unity_env.brain_names[0]\n",
    "        self.observation_space_size = unity_env.brains[self.default_brain].vector_observation_space_size\n",
    "        self.action_space_size = int(unity_env.brains[self.default_brain].vector_action_space_size[0])\n",
    "        \n",
    "    def reset(self, train_mode=True, episode=None):\n",
    "        config = {\"episode\": episode} if episode != None else None\n",
    "        self.env_info = self.env.reset(train_mode=train_mode, config=config)[self.default_brain]\n",
    "        return self.env_info.vector_observations[0]\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.env_info = self.env.step(action)[self.default_brain]\n",
    "        next_state = self.env_info.vector_observations[0]\n",
    "        reward = self.env_info.rewards[0]\n",
    "        done = self.env_info.local_done[0]\n",
    "        return next_state, reward, done, self.env_info\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "env = GymEnv(unity_env)\n",
    "action_size = env.action_space_size\n",
    "state_size = env.observation_space_size\n",
    "state = env.reset()\n",
    "\n",
    "print('Number of actions:', action_size)\n",
    "print('States have length:', state_size)\n",
    "print('States look like:', state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the Reinforcement Learning Components\n",
    "\n",
    "The next step is to define the components that make up the reinforcement learning algorithm. This requires defining:\n",
    "\n",
    "- **3.1** The neural network architecture to approximate the Q function.\n",
    "- **3.2** The experience replay buffer for storing and sampling batches of experience tuples.\n",
    "- **3.3** The Q-Learning agent that samples from the replay buffer and trains the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.0 Neural Network Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 800\n",
    "LEARNING_RATE = 0.0001\n",
    "REGULARIZER_LAMBDA = 1e-6\n",
    "TARGET_UPDATE_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3.1.1 Tensorflow QNetwork\n",
    "\n",
    "Below is the implementation of a QNetwork using tensorflow as the backend model. This contains a tensorflow session and after initially building the network graph, it is trained by running the optimizer with the state, action and q_target values for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow: 2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow:\", tf.__version__)\n",
    "\n",
    "class TFModel(tf.keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        self.hidden1 = tf.keras.layers.Dense(HIDDEN_SIZE, activation=tf.nn.relu, kernel_initializer=tf.initializers.glorot_normal())\n",
    "        self.hidden2 = tf.keras.layers.Dense(HIDDEN_SIZE, activation=tf.nn.relu, kernel_initializer=tf.initializers.glorot_normal(), kernel_regularizer=tf.keras.regularizers.l2(l=REGULARIZER_LAMBDA))\n",
    "        self.q_state = tf.keras.layers.Dense(action_size, activation=None, kernel_initializer=tf.initializers.glorot_normal())\n",
    "        \n",
    "    def call(self, state):\n",
    "        hidden1 = self.hidden1(state)\n",
    "        hidden2 = self.hidden2(hidden1) + hidden1\n",
    "        q_state = self.q_state(hidden2)\n",
    "        return q_state\n",
    "    \n",
    "class TFQNetwork():\n",
    "    def __init__(self, state_size, action_size, load=False):\n",
    "        self.model_local = TFModel(state_size, action_size)\n",
    "        self.model_target = TFModel(state_size, action_size)\n",
    "        self.optimizer = tf.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "        self.action_size = action_size\n",
    "        if load: self.load_model()\n",
    "        \n",
    "    def get_q_state(self, state, use_target=False):\n",
    "        model = self.model_local if not use_target else self.model_target\n",
    "        return model(np.array(state)).numpy()\n",
    "    \n",
    "    def get_loss(self, states, actions, q_targets):\n",
    "        actions_one_hot = tf.one_hot(actions, depth=self.action_size)\n",
    "        q_states = tf.cast(self.model_local(states), tf.float32)\n",
    "        q_states_actions = tf.reduce_sum(tf.multiply(q_states, actions_one_hot), axis=1)\n",
    "        loss = tf.reduce_sum(tf.square(q_states_actions - q_targets))\n",
    "        return loss\n",
    "    \n",
    "    def optimize(self, states, actions, q_targets):\n",
    "        loss = lambda: self.get_loss(states, actions, q_targets)\n",
    "        self.optimizer.minimize(loss=loss, var_list=self.model_local.trainable_weights)\n",
    "        self.soft_copy(self.model_local, self.model_target)\n",
    "        \n",
    "    def soft_copy(self, local, target, tau=TARGET_UPDATE_RATE):\n",
    "        new_target_vars = [t + tau*(l-t) for l,t in zip(local.get_weights(), target.get_weights())]\n",
    "        self.model_target.set_weights(new_target_vars)\n",
    "        \n",
    "    def save_model(self, filepath=\"./saved_models/tensorflow2/model.tf\"):\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        self.model_local.save_weights(filepath)\n",
    "        \n",
    "    def load_model(self, filepath=\"./saved_models/tensorflow2/model.tf\"):\n",
    "        if os.path.exists(filepath + \".index\"):\n",
    "            self.model_local.load_weights(filepath)\n",
    "            self.model_target.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 PyTorch QNetwork\n",
    "\n",
    "Below is the implementation of a QNetwork using pytorch as the backend model. This involves first defining a Model class which subclasses the pytorch nn.Module class and then defines the network graph which can be run with the forward function.\n",
    "\n",
    "Then the Model is included in an enclosing PTQNetwork class which trains the model by taking in the states and running the Model class to get the q values which are then indexed by the actions and then the gradients are calculated from the MSE loss between the predicted q value and the q_target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 0.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "\n",
    "class PTModel(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        self.hidden1 = torch.nn.Linear(state_size, HIDDEN_SIZE)\n",
    "        self.hidden2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        self.q_state = torch.nn.Linear(HIDDEN_SIZE, action_size)\n",
    "        torch.nn.init.xavier_normal_(self.hidden1.weight)\n",
    "        torch.nn.init.xavier_normal_(self.hidden2.weight)\n",
    "        torch.nn.init.xavier_normal_(self.q_state.weight)\n",
    "\n",
    "    def forward(self, state):\n",
    "        hidden1 = torch.nn.functional.relu(self.hidden1(state))\n",
    "        hidden2 = torch.nn.functional.relu(self.hidden2(hidden1)) + hidden1\n",
    "        q_state = self.q_state(hidden2)\n",
    "        return q_state\n",
    "\n",
    "class PTQNetwork():\n",
    "    def __init__(self, state_size, action_size, load=False):\n",
    "        self.model_local = PTModel(state_size, action_size)\n",
    "        self.model_target = PTModel(state_size, action_size)\n",
    "        self.optimizer = torch.optim.Adam(self.model_local.parameters(), lr=LEARNING_RATE, weight_decay=REGULARIZER_LAMBDA)\n",
    "        if load: self.load_model()\n",
    "\n",
    "    def get_q_state(self, state, use_target=False):\n",
    "        model = self.model_local if not use_target else self.model_target\n",
    "        state = torch.from_numpy(np.array(state)).float()\n",
    "        return model(state).detach().numpy()\n",
    "    \n",
    "    def get_loss(self, states, actions, q_targets):\n",
    "        states = torch.from_numpy(np.vstack(states)).float()\n",
    "        actions = torch.from_numpy(np.vstack(actions)).long()\n",
    "        q_targets = torch.from_numpy(np.vstack(q_targets)).float()\n",
    "        q_states_actions = self.model_local(states).gather(1, actions)\n",
    "        loss = (q_states_actions - q_targets)**2\n",
    "        return loss.mean()\n",
    "    \n",
    "    def optimize(self, states, actions, q_targets):\n",
    "        loss = self.get_loss(states, actions, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.soft_copy(self.model_local, self.model_target)\n",
    "        \n",
    "    def soft_copy(self, local, target, tau=TARGET_UPDATE_RATE):\n",
    "        for l,t in zip(local.parameters(), target.parameters()):\n",
    "            t.data.copy_(t.data + tau*(l.data - t.data))\n",
    "        \n",
    "    def save_model(self, filepath=\"./saved_models/pytorch/checkpoint.pth\"):\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        torch.save(self.model_local.state_dict(), filepath)\n",
    "        \n",
    "    def load_model(self, filepath=\"./saved_models/pytorch/checkpoint.pth\"):\n",
    "        if os.path.exists(filepath):\n",
    "            self.model_local.load_state_dict(torch.load(filepath))\n",
    "            self.model_target.load_state_dict(torch.load(filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Experience Replay\n",
    "\n",
    "Below is the implementation of a Replay Buffer using the deque collection as the rolling buffer of experience tuples. This can be sampled by specifying the sample size and then returns each individual experience type as separate lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, maxlen):\n",
    "        self.buffer = deque(maxlen=maxlen)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        sample_size = min(len(self.buffer), batch_size)\n",
    "        samples = random.choices(self.buffer, k=sample_size)\n",
    "        return map(list, zip(*samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Q-Learning Algorithm\n",
    "\n",
    "Below is the implementation of the agent that uses Deep Q Networks to learn the Bellman equation for selecting actions to take in a given state from the environment. It selects an action from an epsilon-greedy policy starting with eps = 1.0 which decreases as the network is trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.0 Q-Learning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_BUFFER_SIZE = 1000000\n",
    "REPLAY_BATCH_SIZE = 32\n",
    "DISCOUNT_RATE = 0.99\n",
    "EPS_MAX = 1.0\n",
    "EPS_MIN = 0.1\n",
    "EPS_DECAY = 0.998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, state_size, action_size, network=TFQNetwork, eps=EPS_MAX, load=False):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.q_network = network(state_size, action_size, load)\n",
    "        self.replay_buffer = ReplayBuffer(MAX_BUFFER_SIZE)\n",
    "        self.eps = eps\n",
    "        self.gamma = DISCOUNT_RATE\n",
    "\n",
    "    def get_action(self, state, eps=None):\n",
    "        eps = self.eps if eps == None else eps\n",
    "        action_greedy = np.argmax(self.q_network.get_q_state([state]))\n",
    "        action_random = np.random.randint(self.action_size)\n",
    "        action = action_random if random.random() < eps else action_greedy\n",
    "        return action\n",
    "        \n",
    "    def train(self, state, action, next_state, reward, done):\n",
    "        self.replay_buffer.add((state, action, next_state, reward, done))\n",
    "        states, actions, next_states, rewards, dones = self.replay_buffer.sample(REPLAY_BATCH_SIZE)\n",
    "        \n",
    "        next_actions = np.argmax(self.q_network.get_q_state(next_states, use_target=False), axis=1)\n",
    "        q_next_states = self.q_network.get_q_state(next_states, use_target=True)\n",
    "        q_next_states[dones] = np.zeros([self.action_size])\n",
    "        q_next_states_next_actions = q_next_states[np.arange(next_actions.shape[0]), next_actions]\n",
    "        q_targets = rewards + self.gamma * q_next_states_next_actions\n",
    "        self.q_network.optimize(states, actions, q_targets)\n",
    "\n",
    "        if done: self.eps = max(self.eps * EPS_DECAY, EPS_MIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Agent\n",
    "\n",
    "Below is the training loop for training the agent through a number of episodes of interacting with the environment. It keeps track of the total reward from each episode and also stores the last 100 episode rewards for calculating the average reward for checking when the environment was solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Score: -44.0, Avg reward: -44.00\n",
      "Episode: 1, Score: -44.0, Avg reward: -44.00\n",
      "Episode: 2, Score: -46.0, Avg reward: -44.67\n",
      "Episode: 3, Score: -46.0, Avg reward: -45.00\n",
      "Episode: 4, Score: -48.0, Avg reward: -45.60\n",
      "Episode: 5, Score: -48.0, Avg reward: -46.00\n",
      "Episode: 6, Score: -46.0, Avg reward: -46.00\n",
      "Episode: 7, Score: -42.0, Avg reward: -45.50\n",
      "Episode: 8, Score: -32.0, Avg reward: -44.00\n",
      "Episode: 9, Score: -42.0, Avg reward: -43.80\n",
      "Episode: 10, Score: -40.0, Avg reward: -43.45\n",
      "Episode: 11, Score: -46.0, Avg reward: -43.67\n",
      "Episode: 12, Score: -46.0, Avg reward: -43.85\n",
      "Episode: 13, Score: -48.0, Avg reward: -44.14\n",
      "Episode: 14, Score: -48.0, Avg reward: -44.40\n",
      "Episode: 15, Score: -48.0, Avg reward: -44.62\n",
      "Episode: 16, Score: -48.0, Avg reward: -44.82\n",
      "Episode: 17, Score: -42.0, Avg reward: -44.67\n",
      "Episode: 18, Score: -44.0, Avg reward: -44.63\n",
      "Episode: 19, Score: -42.0, Avg reward: -44.50\n",
      "Episode: 20, Score: -48.0, Avg reward: -44.67\n",
      "Episode: 21, Score: -46.0, Avg reward: -44.73\n",
      "Episode: 22, Score: -42.0, Avg reward: -44.61\n",
      "Episode: 23, Score: -48.0, Avg reward: -44.75\n",
      "Episode: 24, Score: -48.0, Avg reward: -44.88\n",
      "Episode: 25, Score: -48.0, Avg reward: -45.00\n",
      "Episode: 26, Score: -46.0, Avg reward: -45.04\n",
      "Episode: 27, Score: -42.0, Avg reward: -44.93\n",
      "Episode: 28, Score: -42.0, Avg reward: -44.83\n",
      "Episode: 29, Score: -46.0, Avg reward: -44.87\n",
      "Episode: 30, Score: -46.0, Avg reward: -44.90\n",
      "Episode: 31, Score: -48.0, Avg reward: -45.00\n",
      "Episode: 32, Score: -48.0, Avg reward: -45.09\n",
      "Episode: 33, Score: -44.0, Avg reward: -45.06\n",
      "Episode: 34, Score: -48.0, Avg reward: -45.14\n",
      "Episode: 35, Score: -46.0, Avg reward: -45.17\n",
      "Episode: 36, Score: -48.0, Avg reward: -45.24\n",
      "Episode: 37, Score: -40.0, Avg reward: -45.11\n",
      "Episode: 38, Score: -48.0, Avg reward: -45.18\n",
      "Episode: 39, Score: -48.0, Avg reward: -45.25\n",
      "Episode: 40, Score: -46.0, Avg reward: -45.27\n",
      "Episode: 41, Score: -40.0, Avg reward: -45.14\n",
      "Episode: 42, Score: -46.0, Avg reward: -45.16\n",
      "Episode: 43, Score: -48.0, Avg reward: -45.23\n",
      "Episode: 44, Score: -48.0, Avg reward: -45.29\n",
      "Episode: 45, Score: -48.0, Avg reward: -45.35\n",
      "Episode: 46, Score: -46.0, Avg reward: -45.36\n",
      "Episode: 47, Score: -36.0, Avg reward: -45.17\n",
      "Episode: 48, Score: -48.0, Avg reward: -45.22\n",
      "Episode: 49, Score: -48.0, Avg reward: -45.28\n",
      "Episode: 50, Score: -48.0, Avg reward: -45.33\n",
      "Episode: 51, Score: -44.0, Avg reward: -45.31\n",
      "Episode: 52, Score: -46.0, Avg reward: -45.32\n",
      "Episode: 53, Score: -46.0, Avg reward: -45.33\n",
      "Episode: 54, Score: -48.0, Avg reward: -45.38\n",
      "Episode: 55, Score: -48.0, Avg reward: -45.43\n",
      "Episode: 56, Score: -44.0, Avg reward: -45.40\n",
      "Episode: 57, Score: -48.0, Avg reward: -45.45\n",
      "Episode: 58, Score: -42.0, Avg reward: -45.39\n",
      "Episode: 59, Score: -46.0, Avg reward: -45.40\n",
      "Episode: 60, Score: -48.0, Avg reward: -45.44\n",
      "Episode: 61, Score: -48.0, Avg reward: -45.48\n",
      "Episode: 62, Score: -48.0, Avg reward: -45.52\n",
      "Episode: 63, Score: -38.0, Avg reward: -45.41\n",
      "Episode: 64, Score: -48.0, Avg reward: -45.45\n",
      "Episode: 65, Score: -48.0, Avg reward: -45.48\n",
      "Episode: 66, Score: -46.0, Avg reward: -45.49\n",
      "Episode: 67, Score: -46.0, Avg reward: -45.50\n",
      "Episode: 68, Score: -46.0, Avg reward: -45.51\n",
      "Episode: 69, Score: -46.0, Avg reward: -45.51\n",
      "Episode: 70, Score: -46.0, Avg reward: -45.52\n",
      "Episode: 71, Score: -48.0, Avg reward: -45.56\n",
      "Episode: 72, Score: -48.0, Avg reward: -45.59\n",
      "Episode: 73, Score: -46.0, Avg reward: -45.59\n",
      "Episode: 74, Score: -46.0, Avg reward: -45.60\n",
      "Episode: 75, Score: -46.0, Avg reward: -45.61\n",
      "Episode: 76, Score: -44.0, Avg reward: -45.58\n",
      "Episode: 77, Score: -32.0, Avg reward: -45.41\n",
      "Episode: 78, Score: -46.0, Avg reward: -45.42\n",
      "Episode: 79, Score: -48.0, Avg reward: -45.45\n",
      "Episode: 80, Score: -46.0, Avg reward: -45.46\n",
      "Episode: 81, Score: -48.0, Avg reward: -45.49\n",
      "Episode: 82, Score: -48.0, Avg reward: -45.52\n",
      "Episode: 83, Score: -48.0, Avg reward: -45.55\n",
      "Episode: 84, Score: -46.0, Avg reward: -45.55\n",
      "Episode: 85, Score: -46.0, Avg reward: -45.56\n",
      "Episode: 86, Score: -46.0, Avg reward: -45.56\n",
      "Episode: 87, Score: -46.0, Avg reward: -45.57\n",
      "Episode: 88, Score: -40.0, Avg reward: -45.51\n",
      "Episode: 89, Score: -48.0, Avg reward: -45.53\n",
      "Episode: 90, Score: -20.0, Avg reward: -45.25\n",
      "Episode: 91, Score: -46.0, Avg reward: -45.26\n",
      "Episode: 92, Score: -48.0, Avg reward: -45.29\n",
      "Episode: 93, Score: -46.0, Avg reward: -45.30\n",
      "Episode: 94, Score: -48.0, Avg reward: -45.33\n",
      "Episode: 95, Score: -42.0, Avg reward: -45.29\n",
      "Episode: 96, Score: -46.0, Avg reward: -45.30\n",
      "Episode: 97, Score: -42.0, Avg reward: -45.27\n",
      "Episode: 98, Score: -40.0, Avg reward: -45.21\n",
      "Episode: 99, Score: -46.0, Avg reward: -45.22\n",
      "Episode: 100, Score: -42.0, Avg reward: -45.20\n",
      "Episode: 101, Score: -48.0, Avg reward: -45.24\n",
      "Episode: 102, Score: -48.0, Avg reward: -45.26\n",
      "Episode: 103, Score: -46.0, Avg reward: -45.26\n",
      "Episode: 104, Score: -48.0, Avg reward: -45.26\n",
      "Episode: 105, Score: -46.0, Avg reward: -45.24\n",
      "Episode: 106, Score: -46.0, Avg reward: -45.24\n",
      "Episode: 107, Score: -48.0, Avg reward: -45.30\n",
      "Episode: 108, Score: -42.0, Avg reward: -45.40\n",
      "Episode: 109, Score: -46.0, Avg reward: -45.44\n",
      "Episode: 110, Score: -48.0, Avg reward: -45.52\n",
      "Episode: 111, Score: -48.0, Avg reward: -45.54\n",
      "Episode: 112, Score: -46.0, Avg reward: -45.54\n",
      "Episode: 113, Score: -46.0, Avg reward: -45.52\n",
      "Episode: 114, Score: -48.0, Avg reward: -45.52\n",
      "Episode: 115, Score: -46.0, Avg reward: -45.50\n",
      "Episode: 116, Score: -36.0, Avg reward: -45.38\n",
      "Episode: 117, Score: -44.0, Avg reward: -45.40\n",
      "Episode: 118, Score: -48.0, Avg reward: -45.44\n",
      "Episode: 119, Score: -42.0, Avg reward: -45.44\n",
      "Episode: 120, Score: -46.0, Avg reward: -45.42\n",
      "Episode: 121, Score: -48.0, Avg reward: -45.44\n",
      "Episode: 122, Score: -46.0, Avg reward: -45.48\n",
      "Episode: 123, Score: -48.0, Avg reward: -45.48\n",
      "Episode: 124, Score: -48.0, Avg reward: -45.48\n",
      "Episode: 125, Score: -46.0, Avg reward: -45.46\n",
      "Episode: 126, Score: -48.0, Avg reward: -45.48\n",
      "Episode: 127, Score: -48.0, Avg reward: -45.54\n",
      "Episode: 128, Score: -48.0, Avg reward: -45.60\n",
      "Episode: 129, Score: -44.0, Avg reward: -45.58\n",
      "Episode: 130, Score: -44.0, Avg reward: -45.56\n",
      "Episode: 131, Score: -46.0, Avg reward: -45.54\n",
      "Episode: 132, Score: -46.0, Avg reward: -45.52\n",
      "Episode: 133, Score: -46.0, Avg reward: -45.54\n",
      "Episode: 134, Score: -44.0, Avg reward: -45.50\n",
      "Episode: 135, Score: -44.0, Avg reward: -45.48\n",
      "Episode: 136, Score: -46.0, Avg reward: -45.46\n",
      "Episode: 137, Score: -48.0, Avg reward: -45.54\n",
      "Episode: 138, Score: -48.0, Avg reward: -45.54\n",
      "Episode: 139, Score: -46.0, Avg reward: -45.52\n",
      "Episode: 140, Score: -48.0, Avg reward: -45.54\n",
      "Episode: 141, Score: -48.0, Avg reward: -45.62\n",
      "Episode: 142, Score: -44.0, Avg reward: -45.60\n",
      "Episode: 143, Score: -42.0, Avg reward: -45.54\n",
      "Episode: 144, Score: -48.0, Avg reward: -45.54\n",
      "Episode: 145, Score: -42.0, Avg reward: -45.48\n",
      "Episode: 146, Score: -32.0, Avg reward: -45.34\n",
      "Episode: 147, Score: -48.0, Avg reward: -45.46\n",
      "Episode: 148, Score: -48.0, Avg reward: -45.46\n",
      "Episode: 149, Score: -38.0, Avg reward: -45.36\n",
      "Episode: 150, Score: -48.0, Avg reward: -45.36\n",
      "Episode: 151, Score: -48.0, Avg reward: -45.40\n",
      "Episode: 152, Score: -48.0, Avg reward: -45.42\n",
      "Episode: 153, Score: -46.0, Avg reward: -45.42\n",
      "Episode: 154, Score: -42.0, Avg reward: -45.36\n",
      "Episode: 155, Score: -32.0, Avg reward: -45.20\n",
      "Episode: 156, Score: -42.0, Avg reward: -45.18\n",
      "Episode: 157, Score: -46.0, Avg reward: -45.16\n",
      "Episode: 158, Score: -48.0, Avg reward: -45.22\n",
      "Episode: 159, Score: -48.0, Avg reward: -45.24\n",
      "Episode: 160, Score: -48.0, Avg reward: -45.24\n",
      "Episode: 161, Score: -48.0, Avg reward: -45.24\n",
      "Episode: 162, Score: -48.0, Avg reward: -45.24\n",
      "Episode: 163, Score: -48.0, Avg reward: -45.34\n",
      "Episode: 164, Score: -48.0, Avg reward: -45.34\n",
      "Episode: 165, Score: -44.0, Avg reward: -45.30\n",
      "Episode: 166, Score: -44.0, Avg reward: -45.28\n",
      "Episode: 167, Score: -48.0, Avg reward: -45.30\n",
      "Episode: 168, Score: -48.0, Avg reward: -45.32\n",
      "Episode: 169, Score: -38.0, Avg reward: -45.24\n",
      "Episode: 170, Score: -46.0, Avg reward: -45.24\n",
      "Episode: 171, Score: -46.0, Avg reward: -45.22\n",
      "Episode: 172, Score: -48.0, Avg reward: -45.22\n",
      "Episode: 173, Score: -46.0, Avg reward: -45.22\n",
      "Episode: 174, Score: -46.0, Avg reward: -45.22\n",
      "Episode: 175, Score: -44.0, Avg reward: -45.20\n",
      "Episode: 176, Score: -48.0, Avg reward: -45.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 177, Score: -48.0, Avg reward: -45.40\n",
      "Episode: 178, Score: -46.0, Avg reward: -45.40\n",
      "Episode: 179, Score: -30.0, Avg reward: -45.22\n",
      "Episode: 180, Score: -48.0, Avg reward: -45.24\n",
      "Episode: 181, Score: -48.0, Avg reward: -45.24\n",
      "Episode: 182, Score: -46.0, Avg reward: -45.22\n",
      "Episode: 183, Score: -48.0, Avg reward: -45.22\n",
      "Episode: 184, Score: -46.0, Avg reward: -45.22\n",
      "Episode: 185, Score: -14.0, Avg reward: -44.90\n",
      "Episode: 186, Score: -44.0, Avg reward: -44.88\n",
      "Episode: 187, Score: -48.0, Avg reward: -44.90\n",
      "Episode: 188, Score: -42.0, Avg reward: -44.92\n",
      "Episode: 189, Score: -44.0, Avg reward: -44.88\n",
      "Episode: 190, Score: -48.0, Avg reward: -45.16\n",
      "Episode: 191, Score: -44.0, Avg reward: -45.14\n",
      "Episode: 192, Score: -48.0, Avg reward: -45.14\n",
      "Episode: 193, Score: -48.0, Avg reward: -45.16\n",
      "Episode: 194, Score: -46.0, Avg reward: -45.14\n",
      "Episode: 195, Score: -44.0, Avg reward: -45.16\n",
      "Episode: 196, Score: -46.0, Avg reward: -45.16\n",
      "Episode: 197, Score: -48.0, Avg reward: -45.22\n",
      "Episode: 198, Score: -40.0, Avg reward: -45.22\n",
      "Episode: 199, Score: -42.0, Avg reward: -45.18\n",
      "Episode: 200, Score: -34.0, Avg reward: -45.10\n",
      "Episode: 201, Score: -46.0, Avg reward: -45.08\n",
      "Episode: 202, Score: -46.0, Avg reward: -45.06\n",
      "Episode: 203, Score: -46.0, Avg reward: -45.06\n",
      "Episode: 204, Score: -40.0, Avg reward: -44.98\n",
      "Episode: 205, Score: -42.0, Avg reward: -44.94\n",
      "Episode: 206, Score: -46.0, Avg reward: -44.94\n",
      "Episode: 207, Score: -48.0, Avg reward: -44.94\n",
      "Episode: 208, Score: 16.0, Avg reward: -44.36\n",
      "Episode: 209, Score: -34.0, Avg reward: -44.24\n",
      "Episode: 210, Score: -40.0, Avg reward: -44.16\n",
      "Episode: 211, Score: -42.0, Avg reward: -44.10\n",
      "Episode: 212, Score: -46.0, Avg reward: -44.10\n",
      "Episode: 213, Score: -42.0, Avg reward: -44.06\n",
      "Episode: 214, Score: -48.0, Avg reward: -44.06\n",
      "Episode: 215, Score: -42.0, Avg reward: -44.02\n",
      "Episode: 216, Score: -48.0, Avg reward: -44.14\n",
      "Episode: 217, Score: -44.0, Avg reward: -44.14\n",
      "Episode: 218, Score: -46.0, Avg reward: -44.12\n",
      "Episode: 219, Score: -46.0, Avg reward: -44.16\n",
      "Episode: 220, Score: -48.0, Avg reward: -44.18\n",
      "Episode: 221, Score: -44.0, Avg reward: -44.14\n",
      "Episode: 222, Score: -42.0, Avg reward: -44.10\n",
      "Episode: 223, Score: -46.0, Avg reward: -44.08\n",
      "Episode: 224, Score: -44.0, Avg reward: -44.04\n",
      "Episode: 225, Score: -48.0, Avg reward: -44.06\n",
      "Episode: 226, Score: -46.0, Avg reward: -44.04\n",
      "Episode: 227, Score: -42.0, Avg reward: -43.98\n",
      "Episode: 228, Score: -48.0, Avg reward: -43.98\n",
      "Episode: 229, Score: -44.0, Avg reward: -43.98\n",
      "Episode: 230, Score: -44.0, Avg reward: -43.98\n",
      "Episode: 231, Score: -48.0, Avg reward: -44.00\n",
      "Episode: 232, Score: -46.0, Avg reward: -44.00\n",
      "Episode: 233, Score: -44.0, Avg reward: -43.98\n",
      "Episode: 234, Score: -46.0, Avg reward: -44.00\n",
      "Episode: 235, Score: -42.0, Avg reward: -43.98\n",
      "Episode: 236, Score: -48.0, Avg reward: -44.00\n",
      "Episode: 237, Score: -42.0, Avg reward: -43.94\n",
      "Episode: 238, Score: -48.0, Avg reward: -43.94\n",
      "Episode: 239, Score: -48.0, Avg reward: -43.96\n",
      "Episode: 240, Score: -48.0, Avg reward: -43.96\n",
      "Episode: 241, Score: -48.0, Avg reward: -43.96\n",
      "Episode: 242, Score: -48.0, Avg reward: -44.00\n",
      "Episode: 243, Score: -48.0, Avg reward: -44.06\n",
      "Episode: 244, Score: -46.0, Avg reward: -44.04\n",
      "Episode: 245, Score: -40.0, Avg reward: -44.02\n",
      "Episode: 246, Score: -42.0, Avg reward: -44.12\n",
      "Episode: 247, Score: -46.0, Avg reward: -44.10\n",
      "Episode: 248, Score: -40.0, Avg reward: -44.02\n",
      "Episode: 249, Score: -44.0, Avg reward: -44.08\n",
      "Episode: 250, Score: -40.0, Avg reward: -44.00\n",
      "Episode: 251, Score: 4.0, Avg reward: -43.48\n",
      "Episode: 252, Score: -46.0, Avg reward: -43.46\n",
      "Episode: 253, Score: -48.0, Avg reward: -43.48\n",
      "Episode: 254, Score: -40.0, Avg reward: -43.46\n",
      "Episode: 255, Score: -46.0, Avg reward: -43.60\n",
      "Episode: 256, Score: -44.0, Avg reward: -43.62\n",
      "Episode: 257, Score: -48.0, Avg reward: -43.64\n",
      "Episode: 258, Score: -48.0, Avg reward: -43.64\n",
      "Episode: 259, Score: -48.0, Avg reward: -43.64\n",
      "Episode: 260, Score: -48.0, Avg reward: -43.64\n",
      "Episode: 261, Score: -48.0, Avg reward: -43.64\n",
      "Episode: 262, Score: -40.0, Avg reward: -43.56\n",
      "Episode: 263, Score: -48.0, Avg reward: -43.56\n",
      "Episode: 264, Score: -42.0, Avg reward: -43.50\n",
      "Episode: 265, Score: -46.0, Avg reward: -43.52\n",
      "Episode: 266, Score: -40.0, Avg reward: -43.48\n",
      "Episode: 267, Score: -36.0, Avg reward: -43.36\n",
      "Episode: 268, Score: -44.0, Avg reward: -43.32\n",
      "Episode: 269, Score: -46.0, Avg reward: -43.40\n",
      "Episode: 270, Score: -48.0, Avg reward: -43.42\n",
      "Episode: 271, Score: -42.0, Avg reward: -43.38\n",
      "Episode: 272, Score: -44.0, Avg reward: -43.34\n",
      "Episode: 273, Score: -46.0, Avg reward: -43.34\n",
      "Episode: 274, Score: -42.0, Avg reward: -43.30\n",
      "Episode: 275, Score: -48.0, Avg reward: -43.34\n",
      "Episode: 276, Score: -40.0, Avg reward: -43.26\n",
      "Episode: 277, Score: -42.0, Avg reward: -43.20\n",
      "Episode: 278, Score: -42.0, Avg reward: -43.16\n",
      "Episode: 279, Score: -40.0, Avg reward: -43.26\n",
      "Episode: 280, Score: -48.0, Avg reward: -43.26\n",
      "Episode: 281, Score: -46.0, Avg reward: -43.24\n",
      "Episode: 282, Score: -44.0, Avg reward: -43.22\n",
      "Episode: 283, Score: -48.0, Avg reward: -43.22\n",
      "Episode: 284, Score: -48.0, Avg reward: -43.24\n",
      "Episode: 285, Score: -46.0, Avg reward: -43.56\n",
      "Episode: 286, Score: -46.0, Avg reward: -43.58\n",
      "Episode: 287, Score: -36.0, Avg reward: -43.46\n",
      "Episode: 288, Score: -42.0, Avg reward: -43.46\n",
      "Episode: 289, Score: -46.0, Avg reward: -43.48\n",
      "Episode: 290, Score: -46.0, Avg reward: -43.46\n",
      "Episode: 291, Score: -42.0, Avg reward: -43.44\n",
      "Episode: 292, Score: -36.0, Avg reward: -43.32\n",
      "Episode: 293, Score: -46.0, Avg reward: -43.30\n",
      "Episode: 294, Score: -48.0, Avg reward: -43.32\n",
      "Episode: 295, Score: -44.0, Avg reward: -43.32\n",
      "Episode: 296, Score: -46.0, Avg reward: -43.32\n",
      "Episode: 297, Score: -44.0, Avg reward: -43.28\n",
      "Episode: 298, Score: -42.0, Avg reward: -43.30\n",
      "Episode: 299, Score: -42.0, Avg reward: -43.30\n",
      "Episode: 300, Score: -46.0, Avg reward: -43.42\n",
      "Episode: 301, Score: -46.0, Avg reward: -43.42\n",
      "Episode: 302, Score: -44.0, Avg reward: -43.40\n",
      "Episode: 303, Score: -44.0, Avg reward: -43.38\n",
      "Episode: 304, Score: -38.0, Avg reward: -43.36\n",
      "Episode: 305, Score: -46.0, Avg reward: -43.40\n",
      "Episode: 306, Score: -46.0, Avg reward: -43.40\n",
      "Episode: 307, Score: -48.0, Avg reward: -43.40\n",
      "Episode: 308, Score: -42.0, Avg reward: -43.98\n",
      "Episode: 309, Score: -46.0, Avg reward: -44.10\n",
      "Episode: 310, Score: -46.0, Avg reward: -44.16\n",
      "Episode: 311, Score: -48.0, Avg reward: -44.22\n",
      "Episode: 312, Score: -46.0, Avg reward: -44.22\n",
      "Episode: 313, Score: -34.0, Avg reward: -44.14\n",
      "Episode: 314, Score: -48.0, Avg reward: -44.14\n",
      "Episode: 315, Score: -34.0, Avg reward: -44.06\n",
      "Episode: 316, Score: -48.0, Avg reward: -44.06\n",
      "Episode: 317, Score: -44.0, Avg reward: -44.06\n",
      "Episode: 318, Score: -48.0, Avg reward: -44.08\n",
      "Episode: 319, Score: -46.0, Avg reward: -44.08\n",
      "Episode: 320, Score: -38.0, Avg reward: -43.98\n",
      "Episode: 321, Score: -46.0, Avg reward: -44.00\n",
      "Episode: 322, Score: -24.0, Avg reward: -43.82\n",
      "Episode: 323, Score: -40.0, Avg reward: -43.76\n",
      "Episode: 324, Score: -44.0, Avg reward: -43.76\n",
      "Episode: 325, Score: -46.0, Avg reward: -43.74\n",
      "Episode: 326, Score: -46.0, Avg reward: -43.74\n",
      "Episode: 327, Score: -46.0, Avg reward: -43.78\n",
      "Episode: 328, Score: -16.0, Avg reward: -43.46\n",
      "Episode: 329, Score: -46.0, Avg reward: -43.48\n",
      "Episode: 330, Score: -32.0, Avg reward: -43.36\n",
      "Episode: 331, Score: -12.0, Avg reward: -43.00\n",
      "Episode: 332, Score: -48.0, Avg reward: -43.02\n",
      "Episode: 333, Score: -42.0, Avg reward: -43.00\n",
      "Episode: 334, Score: -46.0, Avg reward: -43.00\n",
      "Episode: 335, Score: -46.0, Avg reward: -43.04\n",
      "Episode: 336, Score: -24.0, Avg reward: -42.80\n",
      "Episode: 337, Score: -46.0, Avg reward: -42.84\n",
      "Episode: 338, Score: -44.0, Avg reward: -42.80\n",
      "Episode: 339, Score: -48.0, Avg reward: -42.80\n",
      "Episode: 340, Score: -22.0, Avg reward: -42.54\n",
      "Episode: 341, Score: -48.0, Avg reward: -42.54\n",
      "Episode: 342, Score: -46.0, Avg reward: -42.52\n",
      "Episode: 343, Score: -42.0, Avg reward: -42.46\n",
      "Episode: 344, Score: -32.0, Avg reward: -42.32\n",
      "Episode: 345, Score: -44.0, Avg reward: -42.36\n",
      "Episode: 346, Score: -40.0, Avg reward: -42.34\n",
      "Episode: 347, Score: -42.0, Avg reward: -42.30\n",
      "Episode: 348, Score: -42.0, Avg reward: -42.32\n",
      "Episode: 349, Score: -34.0, Avg reward: -42.22\n",
      "Episode: 350, Score: -40.0, Avg reward: -42.22\n",
      "Episode: 351, Score: -42.0, Avg reward: -42.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 352, Score: -46.0, Avg reward: -42.68\n",
      "Episode: 353, Score: -44.0, Avg reward: -42.64\n",
      "Episode: 354, Score: -36.0, Avg reward: -42.60\n",
      "Episode: 355, Score: -32.0, Avg reward: -42.46\n",
      "Episode: 356, Score: -46.0, Avg reward: -42.48\n",
      "Episode: 357, Score: -8.0, Avg reward: -42.08\n",
      "Episode: 358, Score: -44.0, Avg reward: -42.04\n",
      "Episode: 359, Score: -36.0, Avg reward: -41.92\n",
      "Episode: 360, Score: -38.0, Avg reward: -41.82\n",
      "Episode: 361, Score: 36.0, Avg reward: -40.98\n",
      "Episode: 362, Score: -44.0, Avg reward: -41.02\n",
      "Episode: 363, Score: -46.0, Avg reward: -41.00\n",
      "Episode: 364, Score: -44.0, Avg reward: -41.02\n",
      "Episode: 365, Score: -42.0, Avg reward: -40.98\n",
      "Episode: 366, Score: -48.0, Avg reward: -41.06\n",
      "Episode: 367, Score: -32.0, Avg reward: -41.02\n",
      "Episode: 368, Score: -48.0, Avg reward: -41.06\n",
      "Episode: 369, Score: -42.0, Avg reward: -41.02\n",
      "Episode: 370, Score: -46.0, Avg reward: -41.00\n",
      "Episode: 371, Score: -46.0, Avg reward: -41.04\n",
      "Episode: 372, Score: -40.0, Avg reward: -41.00\n",
      "Episode: 373, Score: -48.0, Avg reward: -41.02\n",
      "Episode: 374, Score: -34.0, Avg reward: -40.94\n",
      "Episode: 375, Score: -38.0, Avg reward: -40.84\n",
      "Episode: 376, Score: -40.0, Avg reward: -40.84\n",
      "Episode: 377, Score: -18.0, Avg reward: -40.60\n",
      "Episode: 378, Score: -42.0, Avg reward: -40.60\n",
      "Episode: 379, Score: -30.0, Avg reward: -40.50\n",
      "Episode: 380, Score: -30.0, Avg reward: -40.32\n",
      "Episode: 381, Score: 2.0, Avg reward: -39.84\n",
      "Episode: 382, Score: -24.0, Avg reward: -39.64\n",
      "Episode: 383, Score: -48.0, Avg reward: -39.64\n",
      "Episode: 384, Score: -44.0, Avg reward: -39.60\n",
      "Episode: 385, Score: -18.0, Avg reward: -39.32\n",
      "Episode: 386, Score: -32.0, Avg reward: -39.18\n",
      "Episode: 387, Score: -48.0, Avg reward: -39.30\n",
      "Episode: 388, Score: -10.0, Avg reward: -38.98\n",
      "Episode: 389, Score: -44.0, Avg reward: -38.96\n",
      "Episode: 390, Score: -16.0, Avg reward: -38.66\n",
      "Episode: 391, Score: -40.0, Avg reward: -38.64\n",
      "Episode: 392, Score: -4.0, Avg reward: -38.32\n",
      "Episode: 393, Score: -22.0, Avg reward: -38.08\n",
      "Episode: 394, Score: 4.0, Avg reward: -37.56\n",
      "Episode: 395, Score: -44.0, Avg reward: -37.56\n",
      "Episode: 396, Score: -28.0, Avg reward: -37.38\n",
      "Episode: 397, Score: -18.0, Avg reward: -37.12\n",
      "Episode: 398, Score: -44.0, Avg reward: -37.14\n",
      "Episode: 399, Score: -46.0, Avg reward: -37.18\n",
      "Episode: 400, Score: -2.0, Avg reward: -36.74\n",
      "Episode: 401, Score: 22.0, Avg reward: -36.06\n",
      "Episode: 402, Score: 8.0, Avg reward: -35.54\n",
      "Episode: 403, Score: -18.0, Avg reward: -35.28\n",
      "Episode: 404, Score: -36.0, Avg reward: -35.26\n",
      "Episode: 405, Score: -48.0, Avg reward: -35.28\n",
      "Episode: 406, Score: -38.0, Avg reward: -35.20\n",
      "Episode: 407, Score: -48.0, Avg reward: -35.20\n",
      "Episode: 408, Score: -46.0, Avg reward: -35.24\n",
      "Episode: 409, Score: -6.0, Avg reward: -34.84\n",
      "Episode: 410, Score: 44.0, Avg reward: -33.94\n",
      "Episode: 411, Score: -44.0, Avg reward: -33.90\n",
      "Episode: 412, Score: -40.0, Avg reward: -33.84\n",
      "Episode: 413, Score: 42.0, Avg reward: -33.08\n",
      "Episode: 414, Score: -34.0, Avg reward: -32.94\n",
      "Episode: 415, Score: -48.0, Avg reward: -33.08\n",
      "Episode: 416, Score: -48.0, Avg reward: -33.08\n",
      "Episode: 417, Score: -10.0, Avg reward: -32.74\n",
      "Episode: 418, Score: 0.0, Avg reward: -32.26\n",
      "Episode: 419, Score: -34.0, Avg reward: -32.14\n",
      "Episode: 420, Score: -42.0, Avg reward: -32.18\n",
      "Episode: 421, Score: 34.0, Avg reward: -31.38\n",
      "Episode: 422, Score: -42.0, Avg reward: -31.56\n",
      "Episode: 423, Score: -36.0, Avg reward: -31.52\n",
      "Episode: 424, Score: 12.0, Avg reward: -30.96\n",
      "Episode: 425, Score: -44.0, Avg reward: -30.94\n",
      "Episode: 426, Score: 22.0, Avg reward: -30.26\n",
      "Episode: 427, Score: 4.0, Avg reward: -29.76\n",
      "Episode: 428, Score: -30.0, Avg reward: -29.90\n",
      "Episode: 429, Score: -30.0, Avg reward: -29.74\n",
      "Episode: 430, Score: -38.0, Avg reward: -29.80\n",
      "Episode: 431, Score: -44.0, Avg reward: -30.12\n",
      "Episode: 432, Score: -32.0, Avg reward: -29.96\n",
      "Episode: 433, Score: -4.0, Avg reward: -29.58\n",
      "Episode: 434, Score: 4.0, Avg reward: -29.08\n",
      "Episode: 435, Score: -44.0, Avg reward: -29.06\n",
      "Episode: 436, Score: -44.0, Avg reward: -29.26\n",
      "Episode: 437, Score: -44.0, Avg reward: -29.24\n",
      "Episode: 438, Score: -48.0, Avg reward: -29.28\n",
      "Episode: 439, Score: -8.0, Avg reward: -28.88\n",
      "Episode: 440, Score: 22.0, Avg reward: -28.44\n",
      "Episode: 441, Score: -42.0, Avg reward: -28.38\n",
      "Episode: 442, Score: -48.0, Avg reward: -28.40\n",
      "Episode: 443, Score: 42.0, Avg reward: -27.56\n",
      "Episode: 444, Score: 30.0, Avg reward: -26.94\n",
      "Episode: 445, Score: 32.0, Avg reward: -26.18\n",
      "Episode: 446, Score: -48.0, Avg reward: -26.26\n",
      "Episode: 447, Score: 6.0, Avg reward: -25.78\n",
      "Episode: 448, Score: 44.0, Avg reward: -24.92\n",
      "Episode: 449, Score: 24.0, Avg reward: -24.34\n",
      "Episode: 450, Score: -20.0, Avg reward: -24.14\n",
      "Episode: 451, Score: -34.0, Avg reward: -24.06\n",
      "Episode: 452, Score: -4.0, Avg reward: -23.64\n",
      "Episode: 453, Score: 2.0, Avg reward: -23.18\n",
      "Episode: 454, Score: -40.0, Avg reward: -23.22\n",
      "Episode: 455, Score: -2.0, Avg reward: -22.92\n",
      "Episode: 456, Score: -32.0, Avg reward: -22.78\n",
      "Episode: 457, Score: -22.0, Avg reward: -22.92\n",
      "Episode: 458, Score: 14.0, Avg reward: -22.34\n",
      "Episode: 459, Score: -40.0, Avg reward: -22.38\n",
      "Episode: 460, Score: -40.0, Avg reward: -22.40\n",
      "Episode: 461, Score: 30.0, Avg reward: -22.46\n",
      "Episode: 462, Score: -36.0, Avg reward: -22.38\n",
      "Episode: 463, Score: 0.0, Avg reward: -21.92\n",
      "Episode: 464, Score: 10.0, Avg reward: -21.38\n",
      "Episode: 465, Score: -16.0, Avg reward: -21.12\n",
      "Episode: 466, Score: -22.0, Avg reward: -20.86\n",
      "Episode: 467, Score: -20.0, Avg reward: -20.74\n",
      "Episode: 468, Score: -18.0, Avg reward: -20.44\n",
      "Episode: 469, Score: 36.0, Avg reward: -19.66\n",
      "Episode: 470, Score: -12.0, Avg reward: -19.32\n",
      "Episode: 471, Score: -30.0, Avg reward: -19.16\n",
      "Episode: 472, Score: 4.0, Avg reward: -18.72\n",
      "Episode: 473, Score: -32.0, Avg reward: -18.56\n",
      "Episode: 474, Score: -28.0, Avg reward: -18.50\n",
      "Episode: 475, Score: 46.0, Avg reward: -17.66\n",
      "Episode: 476, Score: 34.0, Avg reward: -16.92\n",
      "Episode: 477, Score: 32.0, Avg reward: -16.42\n",
      "Episode: 478, Score: -4.0, Avg reward: -16.04\n",
      "Episode: 479, Score: -2.0, Avg reward: -15.76\n",
      "Episode: 480, Score: -32.0, Avg reward: -15.78\n",
      "Episode: 481, Score: -8.0, Avg reward: -15.88\n",
      "Episode: 482, Score: 40.0, Avg reward: -15.24\n",
      "Episode: 483, Score: 42.0, Avg reward: -14.34\n",
      "Episode: 484, Score: 14.0, Avg reward: -13.76\n",
      "Episode: 485, Score: -26.0, Avg reward: -13.84\n",
      "Episode: 486, Score: -24.0, Avg reward: -13.76\n",
      "Episode: 487, Score: -20.0, Avg reward: -13.48\n",
      "Episode: 488, Score: 8.0, Avg reward: -13.30\n",
      "Episode: 489, Score: -2.0, Avg reward: -12.88\n",
      "Episode: 490, Score: 26.0, Avg reward: -12.46\n",
      "Episode: 491, Score: 30.0, Avg reward: -11.76\n",
      "Episode: 492, Score: 24.0, Avg reward: -11.48\n",
      "Episode: 493, Score: 0.0, Avg reward: -11.26\n",
      "Episode: 494, Score: -20.0, Avg reward: -11.50\n",
      "Episode: 495, Score: -18.0, Avg reward: -11.24\n",
      "Episode: 496, Score: 8.0, Avg reward: -10.88\n",
      "Episode: 497, Score: -44.0, Avg reward: -11.14\n",
      "Episode: 498, Score: -34.0, Avg reward: -11.04\n",
      "Episode: 499, Score: -24.0, Avg reward: -10.82\n"
     ]
    }
   ],
   "source": [
    "networks = [TFQNetwork, PTQNetwork]\n",
    "agent = DQNAgent(state_size, action_size, network=networks[0], eps=1.0, load=False)\n",
    "\n",
    "scores = []\n",
    "scores_buffer = deque(maxlen=100)\n",
    "avg_scores = []\n",
    "num_episodes = 1000\n",
    "for ep in range(num_episodes):\n",
    "    state = env.reset(True)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.train(state, action, next_state, reward, done)\n",
    "        total_reward += reward  \n",
    "        state = next_state \n",
    "\n",
    "    scores.append(total_reward)\n",
    "    scores_buffer.append(total_reward)\n",
    "    avg_scores.append(np.mean(scores_buffer))\n",
    "    print(\"Episode: {}, Score: {}, Avg reward: {:.2f}\".format(ep, scores[-1], avg_scores[-1]))\n",
    "    \n",
    "    with open(\"./saved_models/train_{}.txt\".format(HIDDEN_SIZE), 'a+') as f:\n",
    "        f.writelines(\"Episode: {}, Score: {}, Avg reward: {:.2f}\\n\".format(ep, scores[-1], avg_scores[-1]))\n",
    "    \n",
    "    if ep % 100 == 99: agent.q_network.save_model(\"./saved_models/temp/{0}/model.tf\".format(ep))    \n",
    "    agent.q_network.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "scores = []\n",
    "scores_buffer = deque(maxlen=100)\n",
    "avg_scores = []\n",
    "\n",
    "with open(\"./saved_models/train_500.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\n')\n",
    "        match = re.match(\"Episode: (.*), Score: (.*),\", line)\n",
    "        if match: \n",
    "            ep = int(match.groups()[0])\n",
    "            total_reward = float(match.groups()[1])\n",
    "            scores.append(total_reward)\n",
    "            scores_buffer.append(total_reward)\n",
    "            avg_scores.append(np.mean(scores_buffer))\n",
    "            print(\"Episode: {}, Score: {}, Avg reward: {:.2f}\".format(ep, scores[-1], avg_scores[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Plotting the training rewards\n",
    "\n",
    "The plot below shows the total reward from each episode (blue) and the average reward over the last 100 episodes (orange). We can see that the agent was able to reach the average reward of 13 after about 600 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d61bdb072c33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavg_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = range(len(scores))\n",
    "plt.plot(x,scores, x,avg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7efee004ada0>,\n",
       " <matplotlib.lines.Line2D at 0x7efee004af60>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efee01164e0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = range(len(scores))\n",
    "plt.plot(x,scores, x,avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Agent\n",
    "\n",
    "The cell below creates a test agent loading the trained weights for a test run of the environment with the trained model. Change the num_episodes variable to run the simulation and calculate the average reward total over multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Score: 58.0\n"
     ]
    }
   ],
   "source": [
    "test_agent = DQNAgent(state_size, action_size, load=True)\n",
    "\n",
    "for ep in range(1):\n",
    "    state = env.reset(train_mode=False)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = test_agent.get_action(state, eps=0.0)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward  \n",
    "        state = next_state \n",
    "    print(\"Episode: {}, Score: {}\".format(ep, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Close the environment when finished\n",
    "When we are finished using an environment, we can close it with the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
